% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{acm_proc_article-me}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{url}

\newcommand\queries{\mathbb{Q}}

\newcommand\refEvidences{\mathbb{E}}
\newcommand\refLabels{\mathbb{L}}
\newcommand\refNames{\mathbb{N}}
\newcommand\shots{\mathbb{S}}

\newcommand\hypEvidences{\mathcal{E}}
\newcommand\hypLabels{\mathcal{L}}
\newcommand\hypNames{\mathcal{N}}

\newcommand\ratio{\rho}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}
%
% --- Author Metadata here ---

%Copyright is held by the author/owner(s).
 
\conferenceinfo{MediaEval 2015 Workshop}{Sept. 14-15, 2015, Wurzen, Germany}

%\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{
Multimodal Person Discovery in Broadcast TV \\
at Mediaeval 2015
}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{1}

\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Johann Poignant, Herv\'e Bredin, Claude Barras \\
       \affaddr{LIMSI - CNRS - Rue John Von Neumann, Orsay, France.}
       \email{firstname.lastname@limsi.fr}
\\
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\date{30 July 2015}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
In this paper, we described the newly "Multimodal Person Discovery in Broadcast TV" task formed part of the MediaEval 2015 evaluation campaign. The purpose of this task is to return the name(s) of people who can be both seen as well as heard for each shots of a video. The list of people is not known a priori and their names must be discovered in an unsupervised way from provided text overlay or speech transcripts. The task will be evaluated using standard information retrieval metrics based on a posteriori collaborative annotation of the corpus.
\end{abstract}

\section{Motivation}

TV archives maintained by national institutions such as the French INA, the Netherlands Institute for Sound \& Vision, or the BBC are rapidly growing in size. The need for applications that make these archives searchable has led researchers to devote concerted effort to developing technologies that create indexes.

Indexes that represent the location and identity of people in the archive are indispensable for searching archives. Human nature leads people to be very interested in other people. However, at the moment that content is created or broadcast, it is not always possible to predict which people will be the most important to find in the future. Someone who appeared in a broadcast, but was relatively unnoticed, might suddenly start generating a buzz and become a trending topic on social networks or search engines. For this reason, it is not possible to assume that a biometric models capable of detecting an individual, will be present at indexing time. For some people such a model may not be available in advance, simply because they are not (yet) famous. In such cases, it is also possible that archivists annotating content by hand do not even know the name of the person. The goal of this task is to address the challenge of indexing people in the archive, under real-world conditions (i.e., there is no pre-set list of people to index). \\

Until very recently, research works dealt only with speaker or face identification. We can mention the works of \textit{Canseco et al.}~\cite{CANSECO--ASRU--2005, CANSECO--INTERSPEECH--2004} as the first paper that did not use biometric models to identify speakers based on pronounced names. Following works improved this idea: \cite{ESTEVE--INTERSPEECH--2007, JOUSSE--ICCASP--2009, MAUCLAIR--Odyssey--2006, TRANTER--ICASSP--2006}. However, in all the above studies, the identification error rate is very high when automatically recognized (and noisy) pronounced names are used as source of naming information. For face identification, in \cite{HOUGHTON--IS--1999, SATOH--IEEEMM--1999, YANG--ACMMM--2004, YANG--ACMMM--2005} they use written names on screen in a title box as source of identities, but the high word error rate on overlaid names transcription greatly limited the use of this source of information.

In 2011 began the REPERE challenge~\cite{BERNARD--SLAM--2013, KAHN--CBMI--2012}. It aimed at supporting research on person recognition in multi-modal conditions. The main objective of this challenge was to answer the two questions \emph{``who speaks when?''} and \emph{``who appears when?''} using any sources of information (including pre-existing biometric models and person names extracted from text overlay and speech transcripts). To assess the technology progress, annual evaluation campaigns were organized from 2012 to 2014. In this context, the REPERE corpus~\cite{GIRAUDEL--LREC--2012} with multi-modal annotation was developed. 3 consortiums composed of multiple teams (including ourselves) participated to this challenge. Thanks to this challenge, new research contributions appeared using the multi-modal aspect (\cite{BECHET--INTERSPEECH--2014, BENDRIS--CBMI--2013, BREDIN--ODYSSEY--2014, BREDIN--INTERSPEECH--2013, BREDIN--SLAM--2013, BREDIN--IJMIR--2014, FAVRE--SLAM--2013, GAY--CBMI--2014, POIGNANT--ASLP--2015, POIGNANT--SLAM--2013, POIGNANT--INTERSPEECH--2012, POIGNANT--MTAP--2015, ROUVIER--CBMI--2014})  .

\section{Definition of the task}

Participants are provided with a collection of TV broadcasts pre-segmented into shots.
Each shot $s \in \shots$ must be automatically tagged with the names of people both speaking and appearing at the same time during the shot: this tagging algorithm is denoted by $\hypLabels : \shots \mapsto \mathcal{P}(\hypNames)$ in the rest of the paper.
The main novelty of the task is that the list of persons is not provided \emph{a priori}, and person models (neither voice nor face) may not be trained on external data. The only way to identify a person is by finding their name $n \in \hypNames$ in the audio (e.g. using speech transcription -- ASR) or visual (e.g. using optical character recognition -- OCR) streams and associating them to the correct person. This makes the task completely unsupervised (i.e. using algorithms not relying on pre-existing labels or biometric models).

\begin{figure}[!htb]
 \center
 \includegraphics[width=1\linewidth]{figs/evidence.png}
 \centering
 \caption {Expected output}
 \label{fig:evidence}
\end{figure}

To ensure participants follow this strict \emph{``no biometric supervision''} constraint, each hypothesized name $n \in \hypNames$ must be backed up by a carefully selected and unique shot prooving that the person actually holds this name $n$: we call this an evidence and denote it by $\hypEvidences : \hypNames \mapsto \shots$. In real-world conditions, this evidence would help a human annotator double-check the automatically-generated index, even for people they did not know beforehand.

Two types of evidence are allowed: an \emph{image} evidence is a shot during which a person is visible, and their name is written on screen; an \emph{audio} evidence is a shot during which a person is visible, and their name is pronounced at least once during a $[\text{shot start time} - 5s, \text{shot end time} + 5s ]$ neighborhood.
For instance, in Figure~\ref{fig:evidence}, shot \#1 is an evidence for Mr A (because his name and his face are visible simultaneously on screen) while shot \#3 is an evidence for Mrs B (because her name is pronounced less than 5 seconds before or after her face is visble on screen).

Because person names are detected and transcribed automatically, they may contain transcription errors to a certain extent (more on that later in Section~\ref{sec:metric}). In the following, we denote by $\refNames$ the set of all possible person names in the universe, correctly formatted as \texttt{firstname\_lastname} (while $\hypNames$ is the set of hypothesized names).

\section{Datasets}

The REPERE corpus -- distributed by ELDA -- serves as development set. It is composed of various TV shows (around news, politics and people) from two French TV channels, for a total of 137 hours. A subset of 50 hours is manually annotated. Audio annotations are dense and provide speech transcripts and identity-labeled speech turns. Video annotations are sparse (one image every 10 seconds) and provide overlaid text transcripts and identity-labeled face segmentation. Both speech and overlaid text transcripts are tagged with named entities.
The test set -- distributed by INA -- contains 106 hours of video, corresponding to 172 editions of evening broadcast news \emph{``Le 20 heures''} of French public channel \emph{``France 2''}, from January 1st 2007 to June 30st 2007. It comes completely free of any annotation.

The test set was annotated \emph{a posteriori} based on participants' submissions. In the following, groundtruth is denoted by function $\refLabels : \shots \mapsto \mathcal{P}(\refNames)$ that maps each shot $s$ to the set of names of every speaking face it contains. Function $\refEvidences : \shots \mapsto \mathcal{P}(\refNames)$ denotes the evidence groundtruth that maps each shot $s$ to the set of person names for which it actually is an evidence.

\section{Baseline and metadata}

This task targets researchers from several communities including multimedia, computer vision, speech and natural language processing. Though the task is multimodal by design and necessitates expertise in various domains, the technological barriers to entry is lowered by the provision of a baseline system described in Figure~\ref{fig:baseline} and available as open-source software~\footnote{\url{http://github.com/MediaEvalPersonDiscovery}}.
For instance, a researcher from the speech processing community could focus its research efforts on improving speaker diarization and automatic speech transcription, while still being able to rely on provided face detection and tracking results to participate to the task.

\begin{figure}[htb]
 \center
 \includegraphics[width=1\linewidth]{figs/baseline.png}
 \centering
 \caption {Multimodal baseline pipeline. Output of greyed out modules is available to participants.}
 \label{fig:baseline}
\end{figure}

The audio stream is segmented into speech turns, while faces are detected and tracked in the visual stream.
Speech turns (resp. face tracks) are then compared and clustered based on MFCC and the Bayesian Information Criterion~\cite{CHEN--DARPA--1998} (resp. HOG\cite{DALAL--CVPR--2005} and Logistic Discriminant Metric Learning\cite{GUILLAUMIN--JCV--2012} on facial landmarks\cite{URICAR--VISAPP--2012}). The approach proposed in~\cite{POIGNANT--MTAP--2015} is also used to compute a probabilistic mapping between co-occuring faces and speech turns. Written (resp. pronounced) person names are automatically extracted from the visual stream (resp. the audio stream) using open source LOOV Optical Character Recognition~\cite{POIGNANT--ICME--2012} (resp. Automatic Speech Recognition~\cite{LAMEL--IWSLT--2011, DIANRELLI--IJCNLP--2011}). The fusion module is a two-steps algorithm: propagation of written names onto speaker clusters~\cite{POIGNANT--INTERSPEECH--2012} followed by propagation of speaker names onto co-occurring speaking faces.

\section{Evaluation metric}
\label{sec:metric}

As the task aims an information retrieval use case, we used a variation of the classical Mean Average Precision (MAP) that integrate the correctness evaluation of evidences.

For each query $q \in \queries \subset \refNames$ (\texttt{firstname\_lastname}), the hypothesized person name $n_q$ with the highest Levenshtein ratio $\rho$ to the query $q$ is selected ($\ratio : \refNames \times \hypNames \mapsto [0, 1]$) -- allowing approximate name transcription:
\begin{align}
\displaystyle n_q & =\argmax_{n \in \hypNames} \rho\left(q, n \right) \\
\intertext{Average precision $\text{AP}(q)$ is then computed classically~\cite{REF} based on relevant and returned shots:}
\text{relevant}(q) & = \{ s \in \shots \;|\; q \in \refLabels(s) \} \\
\text{returned}(q) & = {\{ s \in \shots \;|\; n_q \in \hypLabels(s) \}}_{\begin{subarray}{l}\text{sorted by}\\
    \text{confidence}\end{subarray}} \\
\intertext{Evidence correctness $C(q)$ equals the Levenshtein ratio if the hypothesized evidence actually is an evidence for query $q$:}
             C(q) & =
                        \begin{cases}
                            \rho \left(q, n_q \right) & \text{if } n_q \in \refEvidences(\hypEvidences(n_q)) \\
                            0                         & \text{otherwise}
                        \end{cases} \\
\intertext{To ensure participants do provide correct evidences for every hypothesized name $n \in \hypNames$, standard MAP (Mean Average Precision) is altered into EwMAP (Evidence-weighted Mean Average Precision), the official metric for the task:}
            \text{EwMAP} & = \frac{1}{|\queries|} \sum_{q \in \queries} C(q) \cdot \text{AP}(q)
\end{align}

\section{Acknowledgments}

INA \\
ELDA  <--- on les a (beaucoup) payé, quand même !!! donc bon... \\
CAMOMILE  <---- \\

\newpage

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{publi}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references


\end{document}
